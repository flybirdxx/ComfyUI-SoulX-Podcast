一、 SoulX-Podcast ComfyUI 节点开发文档
1. 项目目标
将 soulx-podcast 仓库的核心功能（长篇、多Suno、多方言的播客语音生成）封装为 ComfyUI 自定义节点，实现可视化的播客生成流程。

2. 核心架构分析
根据 soulx-podcast 的技术报告 和核心代码 soulxpodcast/models/soulxpodcast.py，其管线分为三个主要阶段：

LLM 阶段 (Qwen3-1.7B)：将输入的文本（包括提示词、对话脚本、方言标签等）转换为语义标记 (Semantic Tokens)。

Flow 阶段 (CFM)：将 LLM 生成的语义标记，结合Suno（通过提示音频提取）和上下文，解码为声学特征（梅尔频谱图，Mel Spectrogram）。

Vocoder 阶段 (HiFT)：将声学特征转换为最终的 24kHz 音频波形 (Waveform)。

3. 节点设计方案
为了在 ComfyUI 中复现这个流程，我们建议设计以下三个核心节点：

节点一：SoulXPodcastLoader (模型加载器)
功能：一次性加载所有必需的模型和分词器，打包成一个 SOULX_MODEL 对象，供后续节点使用。这能避免重复加载，提高效率。

输入 (INPUT_TYPES)：

model_path (STRING)：指向 pretrained_models/SoulX-Podcast-1.7B 或方言模型的路径。

输出 (RETURN_TYPES)：

SOULX_MODEL (SOULX_MODEL)：一个包含所有已加载模型的 Python 对象。

核心逻辑：

加载 s3tokenizer (speech_tokenizer_v2_25hz)。

加载 LLM (HuggingFace Transformers AutoModelForCausalLM)。

加载 Flow 模型 (CausalMaskedDiffWithXvec)，并加载 flow.pt 权重。

加载 HiFT 声码器 (HiFTGenerator)，并加载 hift.pt 权重。

加载Suno提取模型 (campplus.onnx)。

将这些模型实例全部存入一个字典或类中，作为 SOULX_MODEL 输出。

节点二：SoulXPodcastInputParser (播客输入处理器)
功能：替代原始的 script_xxx.json 和繁琐的数据预处理，提供可视化的输入界面。

输入 (INPUT_TYPES)：

SOULX_MODEL (SOULX_MODEL)：从 SoulXPodcastLoader 传入。

S1_prompt_audio (AUDIO)：Suno1 的提示音频。

S1_prompt_text (STRING)：Suno1 的提示文本。

S2_prompt_audio (AUDIO)：Suno2 的提示音频。

S2_prompt_text (STRING)：Suno2 的提示文本。

dialogue_script (STRING, multiline=True)：对话脚本，例如：

[S1] 你好啊，小希。<|laughter|>
[S2] 你好，能唠！
[S1] <|sigh|> 今天天气真不错。
S1_dialect_prompt (STRING, optional)：Suno1 的方言引导提示词。

S2_dialect_prompt (STRING, optional)：Suno2 的方言引导提示词。

输出 (RETURN_TYPES)：

PODCAST_INPUT (PODCAST_INPUT)：一个包含了所有预处理完毕的数据字典，准备传入主处理器。

核心逻辑：

实现 soulxpodcast/utils/parser.py 中的 podcast_format_parser 逻辑，解析 dialogue_script 为 text_list 和 spk_list。

加载Suno音频，使用 campplus.onnx 提取Suno嵌入 (spk_emb_for_flow)。

加载Suno音频，提取 log_mel (用于 LLM) 和 mel (用于 Flow)。

使用 LLM 的 tokenizer 将 prompt_text、dialogue_script、dialect_prompt_text 转换为 token_ids。

将所有处理好的数据（mels, spk_emb, token_ids 等）打包成一个字典，作为 PODCAST_INPUT 输出。

节点三：SoulXPodcastGenerate (播客生成器)
功能：执行核心的 forward_longform 推理循环，生成最终音频。

输入 (INPUT_TYPES)：

SOULX_MODEL (SOULX_MODEL)：从 SoulXPodcastLoader 传入。

PODCAST_INPUT (PODCAST_INPUT)：从 SoulXPodcastInputParser 传入。

seed (INT)：随机种子。

temperature (FLOAT)：LLM 采样温度。

repetition_penalty (FLOAT)：LLM 重复惩罚。

... (其他采样参数)

输出 (RETURN_TYPES)：

AUDIO (AUDIO)：生成的完整播客音频。

核心逻辑：

设置随机种子。

调用 audio_tokenizer.quantize 获取Suno的 prompt_speech_tokens。

准备 LLM 的初始输入（包含Suno文本和Suno token）。

开始循环（遍历 PODCAST_INPUT 中的每一句对话）：

LLM 阶段：调用 llm.generate 生成当前句子的语义 token。

Flow 阶段：将 (Suno token + 生成的 token) 输入 flow 模型，生成 generated_mels。

Vocoder 阶段：提取新生成的 mel 部分，输入 hift 模型，生成 wav。

将 wav 存储起来，并更新 LLM 的上下文（inputs.extend(llm_outputs['token_ids'])），为下一句对话做准备。

循环结束：将所有 wav 拼接 (torch.cat)，转换为 ComfyUI 的 AUDIO 格式 (tensor, sample_rate) 并输出。

4. 关键依赖
根据 requirements.txt 和代码分析，必须确保 ComfyUI 环境安装了以下关键 Python 包：

s3tokenizer

diffusers

torch (版本需匹配)

transformers (版本需匹配)

onnxruntime (或 onnxruntime-gpu)

einops

librosa, scipy